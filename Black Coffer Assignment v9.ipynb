{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd83210f",
   "metadata": {},
   "source": [
    "# The Outer Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a2e1be",
   "metadata": {},
   "source": [
    "## Read Links and Store in Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30106a97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install xlrd\n",
    "#!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "6c2a4710",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = True\n",
    "test_paragraph = \"XaXeixoxu XaXxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58d06129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from constants import COUNTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b37dd4c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a44f608",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL\n",
       "0      37  https://insights.blackcoffer.com/ai-in-healthc...\n",
       "1      38  https://insights.blackcoffer.com/what-if-the-c...\n",
       "2      39  https://insights.blackcoffer.com/what-jobs-wil...\n",
       "3      40  https://insights.blackcoffer.com/will-machine-...\n",
       "4      41  https://insights.blackcoffer.com/will-ai-repla..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_work = pd.read_excel('Input file.xlsx')\n",
    "df_work.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c8f105",
   "metadata": {},
   "source": [
    "## Extracting header tag using Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c19f3e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28e170ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import requests\n",
    "\n",
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument(\"--windows-size=1920,1200\")\n",
    "\n",
    "driver = webdriver.Chrome()    \n",
    "\n",
    "\n",
    "for i in range(len(df_work)):\n",
    "#for i in range(5):\n",
    "    link = df_work.loc[i,\"URL\"]\n",
    "    try:\n",
    "        driver.get(link)\n",
    "            #df_work[\"title\"] = driver.title\n",
    "        df_work.loc[i, \"title\"] = \"\"\n",
    "        df_work.loc[i, \"title\"] = driver.title\n",
    "\n",
    "        df_work.loc[i, \"article\"] = \"\"\n",
    "\n",
    "        article = driver.find_elements(By.XPATH, '//p[not(@*)]')  # All <p> tags with no attributes.\n",
    "        for paragraph in article:            # Looking to concatenate all strings in array\n",
    "\n",
    "            df_work.loc[i, \"article\"] = \" \".join([df_work.loc[i, \"article\"], paragraph.text])   #df_work.loc[i, \"article\"] + paragraph.text  \n",
    "    except: \n",
    "         continue\n",
    "driver.quit()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2240c3fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>AI in healthcare to Improve Patient Outcomes -...</td>\n",
       "      <td>Introduction “If anything kills over 10 milli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>What if the Creation is Taking Over the Creato...</td>\n",
       "      <td>Human minds, a fascination in itself carrying...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>What Jobs Will Robots Take From Humans in The ...</td>\n",
       "      <td>Introduction AI is rapidly evolving in the em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>Will Machine Replace The Human in the Future o...</td>\n",
       "      <td>“Anything that could give rise to smarter-tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>Will AI Replace Us or Work With Us? - Blackcof...</td>\n",
       "      <td>“Machine intelligence is the last invention t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL  \\\n",
       "0      37  https://insights.blackcoffer.com/ai-in-healthc...   \n",
       "1      38  https://insights.blackcoffer.com/what-if-the-c...   \n",
       "2      39  https://insights.blackcoffer.com/what-jobs-wil...   \n",
       "3      40  https://insights.blackcoffer.com/will-machine-...   \n",
       "4      41  https://insights.blackcoffer.com/will-ai-repla...   \n",
       "\n",
       "                                               title  \\\n",
       "0  AI in healthcare to Improve Patient Outcomes -...   \n",
       "1  What if the Creation is Taking Over the Creato...   \n",
       "2  What Jobs Will Robots Take From Humans in The ...   \n",
       "3  Will Machine Replace The Human in the Future o...   \n",
       "4  Will AI Replace Us or Work With Us? - Blackcof...   \n",
       "\n",
       "                                             article  \n",
       "0   Introduction “If anything kills over 10 milli...  \n",
       "1   Human minds, a fascination in itself carrying...  \n",
       "2   Introduction AI is rapidly evolving in the em...  \n",
       "3   “Anything that could give rise to smarter-tha...  \n",
       "4   “Machine intelligence is the last invention t...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_work.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae94e7c",
   "metadata": {},
   "source": [
    "### Now the next couple of steps are stemming, stopwords, Positive/Negative keywords and statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c84a92",
   "metadata": {},
   "source": [
    "#### Write to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e605b5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx in df_work.index:\n",
    "    text_file = open(\"./Files/\"+str(df_work[\"URL_ID\"][idx]), \"w\")\n",
    "    text_file.write(str(df_work[\"article\"][idx]))\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aed8c22",
   "metadata": {},
   "source": [
    "For each article\n",
    "1. Loop through all files and remove stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10e6e55",
   "metadata": {},
   "source": [
    "## Files for Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01dba2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to populate 'stopwords' variable\n",
    "#STOPWORDS_FILES = [\"StopWords_DatesandNumbers.txt\",]\n",
    "STOPWORDS_FILES=[\"StopWords_Auditor.txt\", \"StopWords_Currencies.txt\", \"StopWords_DatesandNumbers.txt\",\n",
    "                 \"StopWords_Generic.txt\", \"StopWords_GenericLong.txt\", \"StopWords_Geographic.txt\",\n",
    "                 \"StopWords_Names.txt\"]\n",
    "stopwords = [] #\"million\"\n",
    "#Check next line\n",
    "stopwords_array = []\n",
    "for file in STOPWORDS_FILES:\n",
    "    temp_file = open('StopWords/' + file, \"r\")\n",
    "    stopwords_array.append(temp_file.readlines())\n",
    "    temp_file.close()\n",
    "\n",
    "for stopwords_list in stopwords_array:\n",
    "    for stopword in stopwords_list:\n",
    "        stopwords.append(stopword.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "252997fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n",
      "['in', 'china', 'on', 'december']\n"
     ]
    }
   ],
   "source": [
    "#The latest and greatest\n",
    "processed_articles = []\n",
    "for idx in df_work.index:\n",
    "    article = df_work.loc[idx,\"article\"]\n",
    "\n",
    "    if(test):\n",
    "        article = test_paragraph\n",
    "\n",
    "    if(type(article) is str):\n",
    "        article_words = [word.lower() for word in article.split()]\n",
    "        for stopword in stopwords:       \n",
    "            for word in article_words:\n",
    "                if(stopword == word):\n",
    "                    article_words.remove(stopword)\n",
    "        article = \" \".join(article_words)\n",
    "        processed_articles.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "821db221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_articles[113]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3c32e2",
   "metadata": {},
   "source": [
    "## Positive/Negative Word Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cf160618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "#Read all positive words and output them one by one\n",
    "positive_score = [0]*113\n",
    "file = open('positive-words.txt','r')\n",
    "positive_words = file.readlines()\n",
    "file.close()\n",
    "\n",
    "#print(positive_words)\n",
    "positive_words = [s.strip() for s in positive_words]\n",
    "    \n",
    "for idx in range(113): \n",
    "    words = processed_articles[idx].split()\n",
    "    \n",
    "    if(test):\n",
    "        words = test_paragraph.split()\n",
    "\n",
    "#    print(words)\n",
    "    for word in words:\n",
    "        if (word in positive_words):\n",
    "            positive_score[idx] += 1 \n",
    "            \n",
    "print(positive_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1ac7cf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "#Read all negative words and output them one by one\n",
    "negative_score = [0]*113\n",
    "file = open('negative-words.txt', 'r')\n",
    "negative_words = file.readlines()\n",
    "file.close()\n",
    "\n",
    "negative_words = [s.strip() for s in negative_words]\n",
    "\n",
    "for idx in range(113):\n",
    "    words = processed_articles[idx].split()\n",
    "    \n",
    "    if(test):\n",
    "        words = test_paragraph.split()\n",
    "    \n",
    "    for word in words:\n",
    "        if (word in negative_words):\n",
    "            negative_score[idx] += 1 \n",
    "            \n",
    "print(negative_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "139a722f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825, 0.1428571224489825]\n"
     ]
    }
   ],
   "source": [
    "#Polarity score\n",
    "polarity_score = [0]*113\n",
    "for idx in range(113):\n",
    "    polarity_score[idx] = (positive_score[idx] - negative_score[idx])/(positive_score[idx] + negative_score[idx] + 0.000001)\n",
    "print(polarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0f99e01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0, 7000000.0]\n"
     ]
    }
   ],
   "source": [
    "#Subjectivity score\n",
    "subjectivity_score = [0]*113\n",
    "for idx in range(113):\n",
    "    subjectivity_score[idx] = (positive_score[idx] + negative_score[idx])/(len(processed_articles[idx]) + 0.000001)\n",
    "print(subjectivity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "12e8a601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "['SENTENCE OF LENGHT 4', ' SENTENCE OF LENGTH ONLY 5', ' SENTENCE OF LENGHT ONLY ONLY 6']\n",
      "[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]\n"
     ]
    }
   ],
   "source": [
    "#average sentence length\n",
    "\n",
    "average_sentence_length = [0]*113\n",
    "word_count = [0]*113\n",
    "\n",
    "for idx in range(113):\n",
    "    sentences = processed_articles[idx][0:-1].split('.')\n",
    "    \n",
    "    if(test):\n",
    "        sentences = test_paragraph[0:-1].split('.')\n",
    "        \n",
    "    \n",
    "#    print(sentences) \n",
    "    \n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if(sentence == ''):\n",
    "            break\n",
    "        words = sentence.split()\n",
    "        word_count[idx] += len(words)\n",
    "\n",
    "    average_sentence_length[idx] = (word_count[idx])/len(sentences)\n",
    "print(average_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "d935b489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "### Complex Word Count\n",
    "\n",
    "complex_word_count = [0]*113\n",
    "for idx in range(113):\n",
    "    sentences = processed_articles[idx].split('.')\n",
    "    \n",
    "    if(test):\n",
    "        sentences = test_paragraph.split('.')\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        for word in words:\n",
    "            char_count = 0\n",
    "            for char in word:\n",
    "                if(char == 'a' or char == 'e' or char == 'i' or char == 'o' or char =='u'):\n",
    "                    char_count+=1\n",
    "            if(char_count > 2):\n",
    "                complex_word_count[idx] += 1\n",
    "print(complex_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e6c82ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Already got word count from 'averate sentence length' cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "a4ad936b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n",
      "['XaXeixoxu', 'XaXxx']\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "## Syllable count per word\n",
    "syllable_count = [0]*113\n",
    "word_count_articles = [0]*113\n",
    " \n",
    "for idx in range(113):\n",
    "    words = processed_articles[idx].split()\n",
    "    \n",
    "    if(test):\n",
    "        words = test_paragraph.split()\n",
    "    \n",
    "    print(words)\n",
    "    \n",
    "    for word in words:\n",
    "        syllable_count_word = 0\n",
    "        for char in word:\n",
    "                if(char == 'a' or char == 'e' or char == 'i' or char == 'o' or char =='u'):\n",
    "                    syllable_count_word +=1\n",
    "        word_count_articles[idx] +=1 \n",
    "        syllable_count[idx] = syllable_count[idx] + syllable_count_word\n",
    "    \n",
    "                    \n",
    "    if(word_count_articles[idx]!= 0):\n",
    "        print(syllable_count[idx]/word_count_articles[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a8844216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "#Personal pronouns count “I,” “we,” “my,” “ours,” and “us”.\n",
    "\n",
    "pronoun_count = [0]*113\n",
    "for idx in range(113):\n",
    "    words = processed_articles[idx].split()\n",
    "    \n",
    "    if(test):\n",
    "        words = test_paragraph.split()\n",
    "    \n",
    "    \n",
    "    for word in words:\n",
    "        #print(word)\n",
    "        if (word == 'I' or word == 'we' or word == 'my' or word == 'ours' or word == 'us'):\n",
    "            pronoun_count[idx] += 1\n",
    "\n",
    "print(pronoun_count) #0 as stopwords had I, we etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "fc0722a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75]\n"
     ]
    }
   ],
   "source": [
    "#Average word length\n",
    "#Sum of the total number of characters in each word/Total number of words\n",
    "\n",
    "word_count = [0]*113\n",
    "char_count = [0]*113\n",
    "avg_word_len = [0]*113\n",
    "for idx in range(113):\n",
    "    words = processed_articles[idx].split()\n",
    "    \n",
    "    if(test):\n",
    "        words = test_paragraph.split()\n",
    "    \n",
    "    for word in words:\n",
    "        char_count[idx] += len(word)\n",
    "        word_count[idx] += 1\n",
    "    if(word_count[idx]!=0):\n",
    "        avg_word_len[idx]+=char_count[idx]/word_count[idx]\n",
    "\n",
    "\n",
    "print(avg_word_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cea507",
   "metadata": {},
   "source": [
    "### Creating Output File Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22a8fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xlwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "31b7d0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_work.loc[1, \"URL_ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8287b329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwt\n",
    "from xlwt import Workbook\n",
    "\n",
    "\n",
    "df_url_id = df_work[\"URL_ID\"]\n",
    "df_url = df_work[\"URL\"]\n",
    "\n",
    "\n",
    "wb = Workbook()\n",
    "\n",
    "output_sheet = wb.add_sheet('Output')\n",
    "\n",
    "output_sheet.write(0, 1, 'URL_ID')\n",
    "output_sheet.write(0, 2, 'URL')\n",
    "output_sheet.write(0, 3, 'POSITIVE SCORE')\n",
    "output_sheet.write(0, 4, 'NEGATIVE SCORE')\n",
    "output_sheet.write(0, 5, 'POLARITY SCORE')\n",
    "output_sheet.write(0, 6, 'SUBJECTIVITY SCORE')\n",
    "output_sheet.write(0, 7, 'AVG SENTENCE LENGTH')\n",
    "output_sheet.write(0, 8, 'PERCENTAGE OF COMPLEX WORDS')\n",
    "output_sheet.write(0, 9, 'FOG INDEX')\n",
    "output_sheet.write(0, 10, 'AVG NUMBER OF WORDS PER SENTENCE')\n",
    "output_sheet.write(0, 11, 'COMPLEX WORD COUNT')\n",
    "output_sheet.write(0, 12, 'WORD COUNT')\n",
    "output_sheet.write(0, 13, 'SYLLABLE PER WORD')\n",
    "output_sheet.write(0, 14, 'PERSONAL PRONOUNS')\n",
    "output_sheet.write(0, 15, 'AVG WORD LENGTH')\n",
    "\n",
    "for idx in range(1, 113):\n",
    "    output_sheet.write(idx, 1, str(df_work.loc[idx, 'URL_ID']))\n",
    "    output_sheet.write(idx, 2, df_work.loc[idx, 'URL'])\n",
    "    output_sheet.write(idx, 3, str(positive_score[idx]))\n",
    "    output_sheet.write(idx, 4, str(negative_score[idx]))\n",
    "    output_sheet.write(idx, 5, str(polarity_score[idx]))\n",
    "    output_sheet.write(idx, 6, str(subjectivity_score[idx]))\n",
    "    output_sheet.write(idx, 7, str(average_sentence_length[idx]))\n",
    "    output_sheet.write(idx, 8, str(complex_word_count[idx]/word_count))\n",
    "    output_sheet.write(idx, 9, str(0.4()))\n",
    "    output_sheet.write(idx, 10, 'AVG NUMBER OF WORDS PER SENTENCE')\n",
    "    output_sheet.write(idx, 11, str(complex_word_count[idx]))\n",
    "    output_sheet.write(idx, 12, str(word_count))\n",
    "    output_sheet.write(idx, 13, 'SYLLABLE PER WORD')\n",
    "    output_sheet.write(idx, 14, str(pronoun_count[idx]))\n",
    "    output_sheet.write(idx, 15, str(avg_word_len[idx]))\n",
    "\n",
    "#for i in range(len(df_work)):\n",
    "# for i in range(10):\n",
    "#     output_sheet.write(i+1, 1, str(df_url_id[i]))\n",
    "#     output_sheet.write(i+1, 2, df_url[i]) #i+1 because at i=0, the heading labels are defined\n",
    "\n",
    "wb.save('output.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781699f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
